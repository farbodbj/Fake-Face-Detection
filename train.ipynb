{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports and constant globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "USING_LARGE = True\n",
    "\n",
    "TRAIN_IMGS_DIRECTORY_SMALL = \"data-small/train/\"\n",
    "VALIDATION_IMGS_DIRECTORY_SMALL = \"data-small/valid/\"\n",
    "\n",
    "TRAIN_IMGS_DIRECTORY_LARGE = \"data-large/train/\"\n",
    "VALIDATION_IMGS_DIRECTORY_LARGE = \"data-large/valid/\"\n",
    "TEST_IMGS_DIRECTORY_LARGE = \"data-large/valid/\"\n",
    "\n",
    "RESCALING_FACTOR = 1./0xFF\n",
    "IMAGE_SIZE = (254, 254)\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "import os\n",
    "\n",
    "data_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "def save_augs(src_dir, dest_dir, count_aug, max_to_aug):\n",
    "\n",
    "        done = 1\n",
    "        directory = os.fsencode(src_dir)\n",
    "        \n",
    "        for file in os.listdir(directory):\n",
    "                if ~os.fsdecode(file).startswith(\"aug\"):\n",
    "                        img = load_img(f\"{src_dir}/{os.fsdecode(file)}\")  # this is a PIL image\n",
    "                        x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "                        x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "                        # the .flow() command below generates batches of randomly transformed images\n",
    "                        # and saves the results to the `preview/` directory\n",
    "                        if done > max_to_aug:\n",
    "                                break\n",
    "                        i = 1\n",
    "                        for batch in data_gen.flow(x, batch_size=1,\n",
    "                                                save_to_dir=dest_dir, save_prefix='aug', save_format='jpg'):\n",
    "                                i += 1\n",
    "                                if i > count_aug:\n",
    "                                        break  # otherwise the generator would loop indefinitely\n",
    "                                \n",
    "                        done += 1\n",
    "                \n",
    "#save_augs(f\"{TRAIN_IMGS_DIRECTORY_LARGE}fake\", f\"{TRAIN_IMGS_DIRECTORY_LARGE}fake\", 4, 2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if USING_LARGE:\n",
    "    # seems like shuffle has a negative effect on RAM usage and may cause OOM    \n",
    "    image_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=RESCALING_FACTOR,\n",
    "    )\n",
    "    \n",
    "    train_dataset = image_gen.flow_from_directory(\n",
    "        directory = TRAIN_IMGS_DIRECTORY_LARGE,\n",
    "        target_size = IMAGE_SIZE,\n",
    "        class_mode = \"binary\",\n",
    "        seed = 0,\n",
    "        shuffle = True,\n",
    "        batch_size = BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    validation_dataset = image_gen.flow_from_directory(\n",
    "        directory = VALIDATION_IMGS_DIRECTORY_LARGE,\n",
    "        target_size = IMAGE_SIZE,\n",
    "        class_mode = \"binary\",\n",
    "        shuffle = False,\n",
    "        seed = 0,\n",
    "        batch_size = BATCH_SIZE\n",
    "    ) \n",
    "\n",
    "\n",
    "    test_dataset = image_gen.flow_from_directory(\n",
    "        directory = TEST_IMGS_DIRECTORY_LARGE,\n",
    "        target_size = IMAGE_SIZE,\n",
    "        class_mode = \"binary\",\n",
    "        shuffle = True,\n",
    "        seed = 0,\n",
    "        batch_size = BATCH_SIZE\n",
    "    ) \n",
    "else:\n",
    "    image_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=RESCALING_FACTOR,\n",
    "        validation_split=0.15\n",
    "    )\n",
    "    train_dataset =  image_gen.flow_from_directory(\n",
    "        directory = TRAIN_IMGS_DIRECTORY_SMALL,\n",
    "        target_size = IMAGE_SIZE,\n",
    "        class_mode = \"binary\",\n",
    "        subset=\"training\",\n",
    "        shuffle = True,\n",
    "        seed = 0,\n",
    "        batch_size = BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    validation_dataset =  image_gen.flow_from_directory(\n",
    "        directory = TRAIN_IMGS_DIRECTORY_SMALL,\n",
    "        target_size = IMAGE_SIZE,\n",
    "        class_mode = \"binary\",\n",
    "        subset=\"validation\",\n",
    "        shuffle = True,\n",
    "        seed = 0,\n",
    "        batch_size = BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    test_dataset = image_gen.flow_from_directory(\n",
    "        directory = VALIDATION_IMGS_DIRECTORY_SMALL,\n",
    "        target_size = IMAGE_SIZE,\n",
    "        class_mode = \"binary\",\n",
    "        seed = 0,\n",
    "        batch_size = BATCH_SIZE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "\n",
    "def visualize_image_dataset(dataset):\n",
    "    plt.figure()    \n",
    "    for data_batch in dataset:\n",
    "    \n",
    "        img_batch, label_batches = data_batch[0], data_batch[1]\n",
    "        \n",
    "        for idx, img in enumerate(img_batch):\n",
    "            plt.subplot(3, 3, idx + 1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(\"Real\" if label_batches[idx] == 0.0 else \"AI\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "        break\n",
    "\n",
    "\n",
    "visualize_image_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "\n",
    "INPUT_SHAPE = (*IMAGE_SIZE, 3)\n",
    "\n",
    "model = tf.keras.models.Sequential([    \n",
    "    tf.keras.Input(shape=(*IMAGE_SIZE, 3)),\n",
    "    tf.keras.applications.DenseNet121(weights=\"imagenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\", include_top=False),\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation = \"relu\"),\n",
    "    Dense(1, activation = \"sigmoid\")\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.BinaryAccuracy(name=\"acc\"),\n",
    "        tf.keras.metrics.FalseNegatives(),\n",
    "        tf.keras.metrics.FalsePositives(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train logic and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.2,patience=3)\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True)\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"checkpoints\",\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs = 16,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=[early_stopping_cb, reduce_lr, checkpoint_cb]\n",
    ")\n",
    "now = datetime.today().strftime('%Y-%m-%d')\n",
    "tf.keras.saving.save_model(model, f\"model_{now}.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
